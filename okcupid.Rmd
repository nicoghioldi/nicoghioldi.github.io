---
title: "OKCupid"
output: html_document
---

```{r}
setwd("C:/Users/Valeria/Desktop/")
train <- read.csv("C:/Users/Ghiol/Desktop/OKCupid/101.csv", stringsAsFactors=T)
test <- read.csv("C:/Users/Ghiol/Desktop/OKCupid/102.csv", stringsAsFactors=F)
View(train)
train<-train[,-c(21:28)]
test<-test[,-c(21:28)]
n = nrow(train)
m = nrow(test)
combi = merge(train, test, all.x=TRUE, all.y=TRUE,sort=FALSE)
```
#Codifica livelli
```{r}
combi$male<- as.factor(combi$male)
combi$essay_link <- as.factor(combi$essay_link)
combi$tech<- as.factor(combi$tech)
combi$technology<- as.factor(combi$technology)
combi$math<- as.factor(combi$math)
combi$computer<- as.factor(combi$computer)
combi$geek<- as.factor(combi$geek)
combi$science <- as.factor(combi$science)
combi$fixing<- as.factor(combi$fixing)
combi$climbing<- as.factor(combi$climbing)
combi$matrix<- as.factor(combi$matrix)
combi$electronic<- as.factor(combi$electronic)
combi$internet<- as.factor(combi$internet)
combi$skiing<- as.factor(combi$skiing)
combi$solving<- as.factor(combi$solving)
combi$nerdy<- as.factor(combi$nerdy)
combi$board<- as.factor(combi$board)
combi$brain<- as.factor(combi$brain)
combi$shawshank<- as.factor(combi$shawshank)
combi$figuring<- as.factor(combi$figuring)
combi$firefly<- as.factor(combi$firefly)
combi$company<- as.factor(combi$company)
combi$build<- as.factor(combi$build)
combi$hobbies<- as.factor(combi$hobbies)
combi$golf<- as.factor(combi$golf)
combi$card<- as.factor(combi$card)
combi$occasionally<- as.factor(combi$occasionally)
combi$guide<- as.factor(combi$guide)
combi$lists<- as.factor(combi$lists)
combi$section<- as.factor(combi$section)
combi$recent<- as.factor(combi$recent)
combi$awesome<- as.factor(combi$awesome)
combi$law<- as.factor(combi$law)
combi$lol<- as.factor(combi$lol)
combi$justice<- as.factor(combi$justice)
combi$im<- as.factor(combi$im)
combi$studying<- as.factor(combi$studying)
combi$animals<- as.factor(combi$animals)
combi$political<- as.factor(combi$political)
combi$artist<- as.factor(combi$artist)
combi$spiritual<- as.factor(combi$spiritual)
combi$student<- as.factor(combi$student)
combi$tattoos<- as.factor(combi$tattoos)
combi$health<- as.factor(combi$health)
combi$education1<- as.factor(combi$education1)
combi$romance<- as.factor(combi$romance)
combi$dont<- as.factor(combi$dont)
combi$adult<- as.factor(combi$adult)
combi$teaching<- as.factor(combi$teaching)
combi$haha<- as.factor(combi$haha)
combi$ipod<- as.factor(combi$ipod)
combi$native<- as.factor(combi$native)
combi$baseball<- as.factor(combi$baseball)
combi$major<- as.factor(combi$major)
combi$lover<- as.factor(combi$lover)
combi$school<- as.factor(combi$school)
combi$psychology<- as.factor(combi$psychology)
combi$loves<- as.factor(combi$loves)
combi$loyal<- as.factor(combi$loyal)
combi$nyc<- as.factor(combi$nyc)
combi$singing<- as.factor(combi$singing)
combi$kids<- as.factor(combi$kids)

```

#Valutiamo la codifica delle variabili
```{r}
#str(combi)

combi$body_type[combi$body_type=="body_type_missing"]<-NA

combi$diet[combi$diet=="diet_missing"]<-NA
combi$drinks[combi$drinks=="drinks_missing"]<-NA
combi$drugs[combi$drugs=="drugs_missing"]<-NA
combi$education[combi$education=="ed_missing"]<-NA
combi$income[combi$income=="missing"]<-NA
combi$offspring[combi$offspring=="kids_missing"]<-NA
combi$pets[combi$pets=="pets_missing"]<-NA
combi$religion[combi$religion=="religion_missing"]<-NA
combi$sign[combi$sign=="sign_missing"]<-NA
combi$smokes[combi$smokes=="smokes_missing"]<-NA
combi$religion_modifer[combi$religion_modifer=="religion_mod_missing"]<-NA
combi$sign_modifer[combi$sign_modifer=="sign_mod_missing"]<-NA
combi$smokes[combi$smokes=="smokes_missing"]<-NA
```

Vediamo dove sono i dati mancanti:
```{r}
library(VIM)
aggr(combi, prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE)
```

```{r}
table(combi$income)
combi$income[which(is.na(combi$income))]<-factor("inc20000")
table(combi$offspring)
combi$offspring[which(is.na(combi$offspring))]<-factor("doesnt_have_kids")
table(combi$religion_modifer)
combi$religion_modifer[which(is.na(combi$religion_modifer))]<-factor("but_not_too_serious_about_it")
#table(combi$diet)
combi$diet[which(is.na(combi$diet))]<-factor("mostly_anything")
#table(combi$sign_modifer)
combi$sign_modifer[which(is.na(combi$sign_modifer))]<-factor("and_its_fun_to_think_about")
#table(combi$religion)     
combi$religion[which(is.na(combi$religion))]<-factor("agnosticism")
#table(combi$pets)
combi$pets[which(is.na(combi$pets))]<-factor("likes_dogs_and_likes_cats")
#table(combi$drugs)
combi$drugs[which(is.na(combi$drugs))]<-factor("never")
#table(combi$sign)
combi$sign[which(is.na(combi$sign))]<-factor("leo")
#table(combi$body_type)
combi$body_type[which(is.na(combi$body_type))]<-factor("average")
#table(combi$smokes)
combi$smokes[which(is.na(combi$smokes))]<-factor("no")
#table(combi$education)
combi$education[which(is.na(combi$education))]<-factor("graduated_from_college_university")
#table(combi$drinks)
combi$drinks[which(is.na(combi$drinks))]<-factor("socially")
sum(is.na(combi))
```



Torniamo al training e al test per poter fare analisi
```{r}
train = combi[1:n,]
test = combi[(n+1):(n+m),-83]

```


Notiamo che alcune variabili sono piene di dati mancanti per cui decidiamo di escluderle dall'analisi. Inoltre tali variabili (a logica) non sembrano essere rilevanti per determinare se un individuo possa appartenere alla classe "STEM". Inolte le informazioni di alcune variabili escluse possono essere ritrovate nelle variabili dummy, ad esempio:
- offsprings con kids
- diet/drugs/body_type/smokes/drinks con health
L'orientamento religioso non consideriamo essere un elemento fondamentale per la classificazione

#Analisi variabili

Distribuzione variabile risposta:
```{r}
library(ggplot2)
par(mfrow=c(1,2))
ggplot(data=train, mapping=aes(x=Class)) + stat_count()
par(mfrow=c(1,1))
table(train$Class)
```
Valutiamo la relazione tra la variabile Class con le altre (che riteniamo più significative):
+ Grafico in cui si valuta la relazione tra età, education e classe di appartenenza
```{r}
ggplot(train, aes(x=Class, y=age , col=education)) +
  geom_jitter(height = 0.05) + 
  geom_smooth() + 
  facet_wrap( ~ education)
```
Sembra ci sia una relazione tra età e livello di educazione. In particolare, tra chi lavora fanno parte della categoria stem chi è nella categoria *working_on_masters_program* , *on_ph_d_program*, *on_space_camp* e *on college_university*.
Tra i laureati (=graduated) invece appartengono maggiormente alla categoria stem *graduated_from_masters_program* e *college_university*. QUalcuno poi in *ph_d_program*, *space_camp*. Poi chi ha lasciato (_dropped college university possiamo intenderlo magari che ha appena concluso l'uni) dall'università appartiene a stem o chi ancora frequenta il *college_university* appartiene alla categoria stem.
E' un pò confuso, ci sono troppi livelli di *education*. 
Quindi in educaton ci sono alcuni livelli (masters_program, college_univiersity, ph_d_program, space_camp) che favoriscono l'appartenenza a Stem.
**VALUTARE SE CONVIENE RAGGRUPPARLI oppure usare solo i livelli che fanno la differenza**
Per quanto riguarda l'età ci sono più osservazioni che tra i 20-35 anni perchè le persone presenti nel dataset hanno un'età media di 32 anni.

```{r}
plot(train$Class~train$age)
plot(train$Class~train$education)
```
Non sembra ci sia tanta variabilità nelle fasce d'età.
Mentre sembra esserci maggiore variabilità nel livelli della variabile *education* (anche se cntiene dati mancanti e dobbiamo capire come gestirli o utilizzare la dummy). 

#correlazione
```{r}
cor(train[,c(1,7,9)])
library(caret)
vars_zv = nearZeroVar(train, freqCut = 95/5, uniqueCut = 10)
vars_zv
train <- train [,-vars_zv] 
```
Le variabili numeriche non sono correlate e quindi non eliminiamo

**FARE ALTRI GRAFICI DI ANALISI ESPLORATIVA**

Per valutare la bontà della nostra previsione (dal momento che nel test non abbiamo la variabile risposta) occorre dividere in training in due parti, una la usiamo come training, l'altra per valutare la previsione.
Dal momento che il dataset è sbilanciato, occorre dividere in modo da creare due dataset perfettamente bilanciati.

#SSelezione con il lasso
```{r}
x=model.matrix(Class~.,train )[,-49]

y=train$Class
set.seed (1)
tr=sample (1: nrow(x), nrow(x)/2)
te=(- tr )
y.test=y[te]

library(glmnet)
lasso.mod =glmnet(x[tr ,],y[tr],alpha =1,family="binomial")

set.seed (1)
cv.out =cv.glmnet (x[tr,],y[tr],alpha =1, family="binomial")
plot(cv.out)
bestlam =cv.out$lambda.1se
lasso.coef=predict (lasso.mod ,s=bestlam ,type="coefficients")
lasso.coef

```



```{r}
n = nrow(train)/2
m = n
set.seed(30)
istrain = sample(c(rep(T,n),rep(F,length=m)))
new_train = train[istrain,]
new_test = train[!istrain,]

table(new_test$Class)
table(new_train$Class)
```

#MODELLO NULLO
```{r}
fit0 = glm(Class ~ 1, new_train,family="binomial")
phat0 = predict(fit0, newdata=new_test, type="response")
yhat0 = ifelse(phat0 > 0.5, "stem", "other")
mean(phat0)
#previsione modello nullo
table(predicted=yhat0, true=new_test$Class)
mean(yhat0==new_test$Class)

library(pROC)
roc_obj_null <- roc(
	response = new_test$Class,
	predictor = phat0,
	levels = c("stem","other")
	)

auc(roc_obj_null)
```
Accuracy del modello nullo 83- Il mdoello nullo senza down-sampling classifica tutte le osservazioni come "other"

#MOdello completo glm ds
```{r}
#new_train<-new_train[,-c(2,3,4,5,6,8,10,12,13,14,15,19,20)]
library(caret)
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 2,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )
ctrl$sampling <- "down"
set.seed(123)
fit.down <- train(Class ~ ., data = new_train, 
              method = "glm", #modello logistico
              metric = "ROC",
              trControl = ctrl)


phat.down = predict(fit.down, new_test,  type = "prob")[,"stem"]
yhat.down = ifelse(phat.down > .5, "stem","other")

table(yhat.down, new_test$Class)
mean(yhat.down==new_test$Class)

roc_obj_com <- roc(
	response = new_test$Class,
	predictor = phat.down,
	levels = c("stem","other")
	)

auc(roc_obj_com)
```


#SELEZIONE DELLE VARIABILI CON ALBERO
```{r}
library(tree)
library("ggplot2") # Graphics engine
library("dplyr") # Better data manipulations
# analysis packages
library("randomForestSRC") # random forests efficiente
library("ggRandomForests") # ggplot2 random forest figures
#default settings : visualizza in stile ggplot l'output di RandomForest
theme_set(theme_bw())
fit = rfsrc(Class~., data=train, 
            nsplit=0, 
            ntree = 1000, 
            importance=TRUE, 
            block.size=1)

plot(gg_vimp(fit))
```



#ALBERO CON VARIABILI PIù IMPORTANTI
```{r}

ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 2,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )
ctrl$sampling <- "down"
set.seed(123)
#status
fit.tree <- train(Class ~ pets+offspring+body_type+diet+drugs+education+where_town+religion+sign+smokes+male+essay_link+tech+technology+math+computer+geek+science+fixing+electronic+internet+psychology+student+spiritual+artist+awesome+build+board+company, data = train, 
              method = "rpart2", 
              metric = "ROC",
              trControl = ctrl)

phat.tree = predict(fit.tree, new_test,  type = "prob")[,"stem"]
yhat.tree = ifelse(phat.tree > .5, "stem","other")

table(yhat.tree, new_test$Class)
mean(yhat.tree==new_test$Class)
roc_obj_tree <- roc(
	response = new_test$Class,
	predictor = phat.tree,
	levels = c("stem","other")
	)

auc(roc_obj_tree)
```

#GLM con variaibli selezionate dall'albero
```{r}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 2,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )
ctrl$sampling <- "down"
set.seed(123)
fit.glm <- train(Class ~ education++religion+income+male+tech+computer+height, data = new_train, 
              method = "glm", 
              metric = "ROC",
              trControl = ctrl)

phat.glm = predict(fit.glm, new_test,  type = "prob")[,"stem"]
yhat.glm = ifelse(phat.glm > .5, "stem","other")

table(yhat.glm, new_test$Class)
mean(yhat.glm==new_test$Class)
roc_obj_glm <- roc(
	response = new_test$Class,
	predictor = phat.glm,
	levels = c("stem","other")
	)

auc(roc_obj_glm)
```

#Treebag 
```{r}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 2,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )
ctrl$sampling <- "down"
set.seed(123)
fit.bag <- train(Class ~ pets+offspring+body_type+diet+drugs+education+where_town+religion+sign+smokes+status+male+essay_link+tech+technology+math+computer+geek+science+fixing+electronic+internet+psychology+student+spiritual+artist+awesome+build+board+company, data = train, 
              method = "treebag", 
              metric = "ROC",
              trControl = ctrl)

phat.bag = predict(fit.bag, new_test,  type = "prob")[,"stem"]
yhat.bag = ifelse(phat.bag > .5, "stem","other")

table(yhat.bag, new_test$Class)
mean(yhat.bag==new_test$Class)
roc_obj_bag <- roc(
	response = new_test$Class,
	predictor = phat.bag,
	levels = c("stem","other")
	)

auc(roc_obj_bag)
phat_bag = predict(fit.bag, newdata=test, type="prob")[,"stem",drop=F]
write.table(file="myokcupid_BAG.txt", phat_bag, row.names = FALSE, col.names = FALSE)
```

#RANDOM FOREST
```{r}

ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 2,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )
ctrl$sampling <- "down"
set.seed(123)
fit.rf <- train(Class ~ education + pets +
religion + smokes + male + where_town + essay_link + tech + computer + geek + science + internet + awesome + student, data = new_train, 
              method = "rf", 
              metric = "ROC",
              trControl = ctrl)

phat.rf = predict(fit.rf, new_test, type="prob")[,"stem"]
yhat.rf = ifelse(phat.rf> .5, "stem","other")

table(yhat.rf, new_test$Class)
mean(yhat.rf==new_test$Class)
library(pROC)
roc_obj_rf <- roc(
	response = new_test$Class,
	predictor = phat.rf,
	levels = c("stem","other")
	)


auc(roc_obj_rf)
```


```{r}

phat = predict(fit.rf, newdata=test, type="prob")[,"stem",drop=F]

write.table(file="myokcupid_rf.txt", phat, row.names = FALSE, col.names = FALSE)
```

